{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3ecf3b",
   "metadata": {},
   "source": [
    "# Tropical Cyclone Return Period Calculation\n",
    "\n",
    "This Python script calculates the return period for tropical cyclones at different locations. The return period is the average time between events of a certain intensity, such as cyclones of a specific strength or wind speed, and is used in risk assessments.\n",
    "\n",
    "This scripts has been developed by Od√©riz (2025), inspired by the work of Bloemendaal et al. (2020).\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "Before running the script, make sure to install the following libraries:\n",
    "\n",
    "- `pandas`: For handling and manipulating the cyclone data.\n",
    "- `numpy`: For mathematical operations and array handling.\n",
    "- `math`: For advanced mathematical functions.\n",
    "- `matplotlib`: For plotting graphs and visualizations.\n",
    "- `random`: For generating random data (if needed for simulations).\n",
    "\n",
    "You can install these libraries using the following command:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124ca76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lon1,lat1 : coordinates location 1\n",
    "    lon2,lat2 : coordinates location 2\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance in km.\n",
    "\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "\n",
    "def import_storm_data(basin, pathSTORM):\n",
    "    \"\"\"\n",
    "    Import and process storm data from multiple files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    basin : str\n",
    "        The basin ID (e.g., 'ATL' for Atlantic).\n",
    "    pathSTORM : str\n",
    "        The path to the directory containing the storm data files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of pandas DataFrames, each containing storm data from a file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an empty list to store dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    # Loop through the 12 data files (from 0 to 11)\n",
    "    for i in range(12):\n",
    "        # Construct the file path dynamically\n",
    "        file_path = f'{pathSTORM}STORM_DATA_IBTRACS_{str(basin)}_1000_YEARS_{i}.txt'\n",
    "        \n",
    "        # Read the data file\n",
    "        data = np.loadtxt(file_path, delimiter=',')\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Apply the incrementing logic\n",
    "        incrementing_array = np.cumsum(np.r_[0, np.diff(df[0]) != 0])\n",
    "        df[0] = incrementing_array\n",
    "\n",
    "        df = df.rename(columns={0: 'season', 1: 'month', 2: 'sid',3: 'iso_time', 4: 'Basin_ID', 5: 'Latitude',\n",
    "                        6: 'Longitude', 7: 'wmo_pres', 8: 'wmo_wind', 9: 'Radius', 10: 'usa_sshs', 11: 'landfall', 12: 'Dist2land'})  \n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def random_dataframes(pathSTORM0,IB_Storms_Per_Year0,nSTORM,basin):\n",
    "    \"\"\"\n",
    "    Select random storm data from multiple files and calculate the estimated number of years for storms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pathSTORM0 : str\n",
    "        The path to the directory containing the storm data files.\n",
    "    IB_Storms_Per_Year0 : float\n",
    "        The average number of storms per year.\n",
    "    nSTORM : int\n",
    "        The number of random storm samples to select.\n",
    "    basin : str\n",
    "        The basin ID (e.g., 'ATL' for Atlantic).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - DataFrame with random storm data.\n",
    "        - Estimated number of years for storms.\n",
    "    \"\"\"\n",
    "\n",
    "    dataframes_0 = import_storm_data(basin, pathSTORM0)\n",
    "    random_combinations = [(random.randint(0, 11), random.randint(0, 1000)) for _ in range(nSTORM)]\n",
    "\n",
    "    df_all_0=pd.DataFrame()\n",
    "\n",
    "    for idx, (num1, num2) in enumerate(random_combinations, start=1):\n",
    "        \n",
    "        df_0 = dataframes_0[num1]\n",
    "        df_0=df_0[df_0['season']==num2]\n",
    "        df_all_0 = pd.concat([df_all_0, df_0], ignore_index=True)\n",
    "\n",
    "    storm_counts_0 = df_all_0.groupby('season')['sid'].nunique().reset_index()\n",
    "    total_storms_0 = storm_counts_0['sid'].sum()\n",
    "    Estimated_Years_STORMS0=total_storms_0/IB_Storms_Per_Year0\n",
    "\n",
    "    return df_all_0,Estimated_Years_STORMS0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f56e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize iteration and parameters for the simulation\n",
    "iteration = 0  # The iteration number (can be incremented during multiple runs)\n",
    "nyear = 6000  # The number of years to simulate or consider\n",
    "nperiod0 = 3  # Number of periods you want to study to select in period_names\n",
    "\n",
    "# Define the different periods or phases of interest\n",
    "period_names = [\n",
    "    'IB1980-2021LANINA.v1', \n",
    "    'IB1980-2021ENSO_NEUTRAL.v1', \n",
    "    'IB1980-2021ELNINO.v1', \n",
    "    'IB1980-2021'\n",
    "]\n",
    "\n",
    "# Number of years in the IB period (1980 to 2021, inclusive)\n",
    "nIB = 2021 - 1980 + 1  # This calculates the number of years from 1980 to 2021\n",
    "\n",
    "# List of Basins for which the data needs to be processed\n",
    "BASINS_0 = ['EP', 'NA', 'NI', 'SI', 'SP', 'WP']  # EP: Eastern Pacific, NA: North Atlantic, etc.\n",
    "\n",
    "# Path to the output directory where results will be stored\n",
    "pathOut = r'D:\\01.Papers\\2024_ENSO_cyclones\\05_return_period/'  # Make sure the path is correct\n",
    "\n",
    "# Folder name (typically used for saving the output or intermediate files)\n",
    "folder_name = 'List_of_Cities.xlsx'  # Define the folder name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db61f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP\n",
      "NA\n",
      "NI\n",
      "SI\n",
      "SP\n",
      "WP\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#==================================================== \n",
    "# Load cities                 \n",
    "#==================================================== \n",
    "df=pd.read_excel(pathOut+folder_name,header=0,keep_default_na=False)\n",
    "latitudes=df['LATITUDE']\n",
    "longitudes=df['LONGITUDE']\n",
    "basins=df['BASIN']\n",
    "names=df['NAME']\n",
    "capitals=df['CAPITAL CITY']\n",
    "\n",
    "radius=111\n",
    "      \n",
    "returnperiods=[2]\n",
    "returnperiods.extend(np.linspace(5,50,10))\n",
    "returnperiods.extend(np.linspace(60,90,4))\n",
    "returnperiods.extend(np.linspace(100,2000,20))\n",
    "\n",
    "#Please check the definition of wind speed! If it's 1-minute sustained wind, use the following: \n",
    "#wind_items=[20,25,30,33,35,40,42,45,50,55,58,60,65,70,75,80,85]\n",
    "#In STORM, the wind speeds are 10-min average, so the Saffir-Simpson category thresholds need to be converted from 1-min to 10-min: \n",
    "wind_items=[20,25,29,30,35,37.6,40,43.4,45,50,51.1,55,60,61.6,65,70,75]\n",
    "\n",
    "period=period_names[nperiod0]\n",
    "\n",
    "path0=r'C:\\Users\\oderizi\\miniconda\\scripts\\STORM/'+period+'/'\n",
    "pathSTORM0=path0+'tracks/'\n",
    "basins_id0=np.load(path0+'BASINLIST_INTERP.npy',allow_pickle=True,encoding='latin1').item()\n",
    "num_storms_IB = sum(1 for v in basins_id0.values() if v) \n",
    "IB_Storms_Per_Year0=num_storms_IB/nIB\n",
    "\n",
    "wind_dict={i:[] for i in range(len(latitudes))}\n",
    "\n",
    "\n",
    "for ibasin in range (0,6):\n",
    "    basin=BASINS_0[ibasin]\n",
    "    print(basin)\n",
    "    data,Estimated_Years_STORMS0=random_dataframes(pathSTORM0,IB_Storms_Per_Year0,nyear,basin)\n",
    "    time,lat,lon,wind=data[data.columns[3]],data[data.columns[5]],data[data.columns[6]],data[data.columns[8]]\n",
    "    del data\n",
    "        \n",
    "    indices=[i for i,x in enumerate(time) if x==0]\n",
    "    indices.append(len(time))\n",
    "    i=0\n",
    "\n",
    "    #loop over all TCs in the dataset\n",
    "    while i<len(indices)-1:\n",
    "        start=indices[i]\n",
    "        end=indices[i+1]\n",
    "        \n",
    "        latslice=lat[start:end].values\n",
    "        lonslice=lon[start:end].values\n",
    "        windslice=wind[start:end].values\n",
    "                \n",
    "        for l in range(len(latitudes)): #for every city\n",
    "            if basins[l]==basin:\n",
    "                lat_loc=latitudes[l]\n",
    "                lon_loc=longitudes[l]\n",
    "                wind_loc=[]\n",
    "                if lon_loc<0.:\n",
    "                    lon_loc+=360        \n",
    "        \n",
    "                for j in range(len(latslice)):\n",
    "                    #calculate the distance between the track and the capital city\n",
    "                    distance=haversine(lonslice[j],latslice[j],lon_loc,lat_loc)\n",
    "                    \n",
    "                    if distance<=radius:\n",
    "                        wind_loc.append(windslice[j])\n",
    "                \n",
    "                if len(wind_loc)>0.:\n",
    "                    if np.max(wind_loc)>=18.:\n",
    "                        wind_dict[l].append(np.max(wind_loc)) #store the maximum wind speed for the TC\n",
    "\n",
    "        i=i+1\n",
    "\n",
    "#################################################\n",
    "\n",
    "#wind_dict=wind_dicts[f'wind_dict_{iterations}'] \n",
    "\n",
    "for i in range(len(wind_dict)):\n",
    "    name=names[i]\n",
    "    city=capitals[i]\n",
    "    df_winds=pd.DataFrame()\n",
    "    \n",
    "    if len(wind_dict[i])>0.:    \n",
    "        df=pd.DataFrame({'Wind': wind_dict[i]})\n",
    "        df['Ranked']=df['Wind'].rank(ascending=0)\n",
    "        df=df.sort_values(by=['Ranked'])\n",
    "        ranklist=df['Ranked'].tolist()\n",
    "        windlist=df['Wind'].tolist() \n",
    "        \n",
    "        rpwindlist=[]  \n",
    "        NAME_city_1=[] \n",
    "        R_period_1=[]\n",
    "        Wind_velocity_1=[]\n",
    "        \n",
    "        NAME_city_2=[] \n",
    "        R_period_2=[]\n",
    "        Wind_velocity_2=[]\n",
    "            \n",
    "        for m in range(len(ranklist)):\n",
    "            weibull=(ranklist[m])/(len(ranklist)+1.) #weibulls plotting formula. This yields the exceendance probability per event set\n",
    "            r=weibull*(len(ranklist)/nyear) #  nyear  Estimated_Years_STORMS0 multiply by the average number of events per year to reach the exceedence probability per year\n",
    "            rpwindlist.append(1./r) #convert to year instead of probability\n",
    "\n",
    "        rpwindlist=rpwindlist[::-1]         \n",
    "        windlist=windlist[::-1] \n",
    "        \n",
    "        for rp in returnperiods:\n",
    "            windint=np.interp(rp,rpwindlist,windlist)\n",
    "            NAME_city_1.append(city) \n",
    "            R_period_1.append(rp)\n",
    "            Wind_velocity_1.append(windint)\n",
    "\n",
    "        for w in wind_items:\n",
    "            rp_int=np.interp(w,windlist,rpwindlist)\n",
    "            NAME_city_2.append(city) \n",
    "            Wind_velocity_2.append(w)\n",
    "            R_period_2.append(rp_int)\n",
    "\n",
    "\n",
    "        df_winds ['Name']= NAME_city_2\n",
    "        df_winds ['Wind_speed']= Wind_velocity_2\n",
    "        df_winds ['RP'] = R_period_2\n",
    "\n",
    "    df_winds.to_csv(pathOut+city+'_'+period+'_'+str(iteration)+'.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STORM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
